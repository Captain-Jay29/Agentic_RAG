# Personalized Learning Path Navigator (Agentic RAG Demo)

## Overview

This project implements a conversational AI agent designed to guide users through a technical learning path, specifically using a Backend Developer roadmap as the knowledge base. It serves as a practical demonstration of building an Agentic RAG (Retrieval-Augmented Generation) system incorporating:

* **Conversational Memory:** Remembering past interactions using Mem0.
* **Hybrid Knowledge Retrieval:** Combining semantic search (ChromaDB + Sentence Transformers) and structured graph traversal (Neo4j) to fetch relevant information.
* **Agentic Framework:** Using LangChain to orchestrate LLM reasoning, tool usage, and memory management.
* **API Access:** Exposing the agent via a FastAPI endpoint.

This project was developed partly as a learning exercise and preparation focused on concepts relevant to AI Memory Systems.

## Features

* Conversational interface via API.
* Answers questions about topics on the Backend Developer roadmap.
* Retrieves details for specific roadmap topics.
* Suggests the next sequential topic or section in the roadmap.
* Uses hybrid retrieval (semantic + graph) to find relevant information.
* Maintains conversational history per user using Mem0.

## Workflow (User Interaction Flow)

1.  **API Request:** A user sends their message and a unique `user_id` to the `/chat` endpoint (FastAPI).
2.  **Memory Retrieval (Mem0):** The system retrieves relevant conversational history associated with the `user_id` from the `AsyncMemory` (Mem0) instance.
3.  **Agent Execution (LangChain):**
    * The LangChain Agent Executor receives the user's message and the retrieved conversational history.
    * The agent (powered by an LLM like GPT-4o mini) analyzes the input and history, deciding whether to respond directly or use a tool.
4.  **Tool Execution:**
    * The agent selects an appropriate tool (e.g., `roadmap_hybrid_search`, `get_next_steps`, `get_topic_details`).
    * The tool executes, calling functions in `retrieval_logic.py`.
    * **Hybrid Retrieval:** If searching, this involves:
        * Querying ChromaDB for semantically similar topics (embedding the query text with Sentence Transformers).
        * Querying Neo4j for structural context (details, options, sequence via `PRECEDES` relationships).
        * Combining and ranking results.
    * The tool returns formatted information (text) to the agent.
5.  **Response Generation (LLM):** The agent synthesizes the information from tool outputs and conversational history (if any) into a final prompt for the LLM. The LLM generates the natural language response.
6.  **Memory Update (Mem0):** The user's message and the agent's final response are added back to the `AsyncMemory` instance for the specific `user_id`.
7.  **API Response:** The FastAPI endpoint returns the agent's generated response to the user.

## Architecture (Highlighting Mem0)

The system uses the following core components:

* **FastAPI:** Serves the web API endpoint (`/chat`).
* **LangChain:** Orchestrates the agent (`AgentExecutor`, prompts, tool integration).
* **LLM (e.g., OpenAI):** Provides the core reasoning and language generation capabilities.
* **Custom Tools (`agent_tools.py`):** LangChain tools wrapping our custom logic.
* **Retrieval Logic (`retrieval_logic.py`):** Contains functions for querying Neo4j, ChromaDB, and performing hybrid retrieval.
* **Neo4j (AuraDB):** Stores the structured roadmap graph (Sections, Topics, `CONTAINS`, `PRECEDES` relationships). Used for sequence, hierarchy, and detailed property lookups.
* **ChromaDB:** Stores vector embeddings of roadmap topics (generated by Sentence Transformers). Used for semantic similarity search.
* **Mem0 (`AsyncMemory`):** Acts as the **conversational memory layer**.
    * It stores the history of user/agent messages associated with specific `user_id`s.
    * Before the agent acts, it queries Mem0 to retrieve relevant past interactions for conversational context.
    * In this implementation, Mem0 manages the *conversational* aspect of memory, while the static *roadmap knowledge* is retrieved separately from Neo4j/ChromaDB via the LangChain tools. Mem0 likely uses its own internal mechanisms (potentially involving LLM calls and vector search, as indicated by logs) to process and retrieve this history.

## Hybrid Retrieval Strategy

The core knowledge retrieval implemented in `retrieval_logic.py` and exposed via the `HybridRoadmapSearchTool` uses a hybrid approach:

1.  **Semantic Search (ChromaDB):** The user's query text is embedded using Sentence Transformers (`all-MiniLM-L6-v2`). ChromaDB finds the top N topics whose stored embeddings are most similar (cosine distance) to the query embedding. This finds topics related by *meaning*.
2.  **Structural Enrichment & Context (Neo4j):**
    * Full details (description, options, etc.) for the semantically retrieved topics are fetched from Neo4j using their unique IDs.
    * If a `current_topic_id` is provided, its details and structural neighbors (preceding/succeeding topics/sections via `PRECEDES` relationships) are also fetched directly from Neo4j.
3.  **Combining:** Results are combined, deduplicated (by `topic_id`), and potentially ranked (e.g., prioritizing topics found semantically, giving a boost if it matches `current_topic_id`).
4.  **Output:** A consolidated list of enriched topic information is returned to the agent.

## Tech Stack

* **Language:** Python (3.9+ recommended)
* **Agent Framework:** LangChain
* **API Framework:** FastAPI
* **LLM:** OpenAI (GPT-4o mini default) via `langchain-openai`
* **Memory:** Mem0 (`mem0ai` library)
* **Vector Database:** ChromaDB (Persistent Client)
* **Embedding Model:** Sentence Transformers (`all-MiniLM-L6-v2` via Hugging Face)
* **Graph Database:** Neo4j (AuraDB Free targetted)
* **API Server:** Uvicorn
* **Environment:** `python-dotenv`

## Setup & Installation

1.  **Clone Repository:**
    ```bash
    git clone <your-repo-url>
    cd <your-repo-directory>
    ```
2.  **Create Virtual Environment:**
    ```bash
    python3 -m venv .agentic_rag
    source .agentic_rag/bin/activate
    ```
3.  **Install Dependencies:**
    ```bash
    # Recommended: Create requirements.txt first if not present
    # pip freeze > requirements.txt
    pip install -r requirements.txt
    # Or install manually:
    # pip install langchain langchain-openai mem0ai fastapi uvicorn[standard] python-dotenv neo4j chromadb sentence-transformers torch
    ```
4.  **Set up Neo4j:**
    * Create a free Neo4j AuraDB instance ([link](https://neo4j.com/cloud/platform/aura-graph-database/)).
    * Note down the URI, Username (`neo4j`), and Password.
5.  **Set up LLM:**
    * Get an API key from your chosen provider (e.g., OpenAI).
6.  **Configure Environment:**
    * Create a `.env` file in the project root.
    * Add your credentials:
        ```dotenv
        OPENAI_API_KEY=your_openai_api_key
        NEO4J_URL=neo4j+s://your_aura_instance_id.databases.neo4j.io
        NEO4J_USER=neo4j
        NEO4J_PASSWORD=your_aura_password
        ```
7.  **Ingest Data:** Run the ingestion scripts *in order*:
    ```bash
    python Phase1/Neo4j/ingest_neo4j.py
    python Phase1/chroma/ingest_chroma.py
    ```
    *Verify script completion and check logs/database counts.*

## Usage

1.  **Start the API Server:**
    ```bash
    python Phase3/agent_main.py
    ```
    *(Ensure you run it from the root directory so imports work correctly, or adjust paths if running from within Phase3).*
    The server will start, typically on `http://localhost:8000`.

2.  **Interact with the Agent:**
    * **Swagger UI:** Open `http://localhost:8000/docs` in your browser for interactive testing.
    * **curl:** Use `curl` or a similar tool to send POST requests to `http://localhost:8000/chat`:
        ```bash
        curl -X POST "http://localhost:8000/chat" \
        -H "Content-Type: application/json" \
        -d '{
          "user_id": "my_test_user",
          "message": "What are NoSQL databases?"
        }'
        ```

## Future Improvements

* Integrate `getzep/graphiti` for dynamic fact extraction from conversations.
* Implement user progress tracking in Neo4j (`UpdateProgressTool`).
* Refine Mem0 configuration for better conversational recall (e.g., tuning search, explicitly storing roles via metadata).
* Explore deeper LangChain memory integration with Mem0 if possible.
* Develop a simple frontend interface.
